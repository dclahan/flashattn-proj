# gpu info
=== GPU Information ===
Name: NVIDIA GeForce RTX 4070
Compute Capability: 8.9
Global Memory: 11.595 GB
Shared Memory per Block: 48 KB
Registers per Block: 65536
Warp Size: 32
Max Threads per Block: 1024
Max Threads per SM: 1536
Number of SMs: 46
=================================

# data
=====================================================
=== Attention Mechanism Benchmark ===
Configuration: B=8, nh=8, N=64, d=64
Total Q size: 1 MB

=== RESULTS ===
Naive Attention:
  Min: 31.206 ms
  Max: 31.938 ms
  Avg: 31.772 ms
Flash Attention:
  Min: 1.368 ms
  Max: 1.636 ms
  Avg: 1.401 ms

Avg Speedup: 22.68x



=====================================================
=== Attention Mechanism Benchmark ===
Configuration: B=16, nh=8, N=64, d=64
Total Q size: 2 MB

=== RESULTS ===
Naive Attention:
  Min: 89.610 ms
  Max: 91.744 ms
  Avg: 91.503 ms
Flash Attention:
  Min: 2.377 ms
  Max: 4.654 ms
  Avg: 2.728 ms

Avg Speedup: 33.54x


=====================================================
=== Attention Mechanism Benchmark ===
Configuration: B=32, nh=8, N=64, d=64
Total Q size: 4 MB

=== RESULTS ===
Naive Attention:
  Min: 289.643 ms
  Max: 289.914 ms
  Avg: 289.735 ms
Flash Attention:
  Min: 3.874 ms
  Max: 6.319 ms
  Avg: 4.573 ms

Avg Speedup: 63.36x

=====================================================
=== Attention Mechanism Benchmark ===
Configuration: B=8, nh=12, N=64, d=64
Total Q size: 1.5 MB

=== RESULTS ===
Naive Attention:
  Min: 59.728 ms
  Max: 62.358 ms
  Avg: 61.910 ms
Flash Attention:
  Min: 2.231 ms
  Max: 3.192 ms
  Avg: 2.334 ms

Avg Speedup: 26.52x

=====================================================
=== Attention Mechanism Benchmark ===
Configuration: B=16, nh=12, N=64, d=64
Total Q size: 3 MB

=== RESULTS ===
Naive Attention:
  Min: 182.154 ms
  Max: 182.377 ms
  Avg: 182.234 ms
Flash Attention:
  Min: 3.593 ms
  Max: 6.449 ms
  Avg: 4.099 ms

Avg Speedup: 44.46x


=====================================================
=== Attention Mechanism Benchmark ===
Configuration: B=32, nh=12, N=64, d=64
Total Q size: 6 MB

=== RESULTS ===
Naive Attention:
  Min: 696.271 ms
  Max: 697.147 ms
  Avg: 696.679 ms
Flash Attention:
  Min: 5.978 ms
  Max: 9.685 ms
  Avg: 7.747 ms

Avg Speedup: 89.93x


=====================================================
=== Attention Mechanism Benchmark ===
Configuration: B=8, nh=16, N=64, d=64
Total Q size: 2 MB

=== RESULTS ===
Naive Attention:
  Min: 90.084 ms
  Max: 91.914 ms
  Avg: 91.591 ms
Flash Attention:
  Min: 2.408 ms
  Max: 4.528 ms
  Avg: 2.774 ms

Avg Speedup: 33.02x

=====================================================
=== Attention Mechanism Benchmark ===
Configuration: B=16, nh=16, N=64, d=64
Total Q size: 4 MB

=== RESULTS ===
Naive Attention:
  Min: 289.524 ms
  Max: 289.912 ms
  Avg: 289.709 ms
Flash Attention:
  Min: 3.868 ms
  Max: 6.322 ms
  Avg: 4.589 ms

Avg Speedup: 63.12x


=====================================================
=== Attention Mechanism Benchmark ===
Configuration: B=32, nh=16, N=64, d=64
Total Q size: 8 MB

=== RESULTS ===
Naive Attention:
  Min: 1136.394 ms
  Max: 1137.242 ms
  Avg: 1136.786 ms
Flash Attention:
  Min: 7.509 ms
  Max: 11.854 ms
  Avg: 10.059 ms

Avg Speedup: 113.01x

## medium


=====================================================
=== Attention Mechanism Benchmark ===
Configuration: B=8, nh=12, N=128, d=64
Total Q size: 3 MB

=== RESULTS ===
Naive Attention:
  Min: 199.131 ms
  Max: 250.201 ms
  Avg: 226.242 ms
Flash Attention:
  Min: 5.517 ms
  Max: 49.216 ms
  Avg: 12.027 ms

Avg Speedup: 18.81x



=====================================================
=== Attention Mechanism Benchmark ===
Configuration: B=16, nh=12, N=128, d=64
Total Q size: 6 MB

=== RESULTS ===
Naive Attention:
  Min: 695.974 ms
  Max: 697.487 ms
  Avg: 696.898 ms
Flash Attention:
  Min: 7.674 ms
  Max: 11.393 ms
  Avg: 9.184 ms

Avg Speedup: 75.88x

=====================================================
=== Attention Mechanism Benchmark ===
Configuration: B=8, nh=8, N=256, d=64
Total Q size: 4 MB

=== RESULTS ===
Naive Attention:
  Min: 289.654 ms
  Max: 289.884 ms
  Avg: 289.752 ms
Flash Attention:
  Min: 10.893 ms
  Max: 13.424 ms
  Avg: 11.387 ms

Avg Speedup: 25.45x

=====================================================
=== Attention Mechanism Benchmark ===
Configuration: B=16, nh=8, N=256, d=64
Total Q size: 8 MB

=== RESULTS ===
Naive Attention:
  Min: 1136.503 ms
  Max: 1137.476 ms
  Avg: 1136.821 ms
Flash Attention:
  Min: 21.442 ms
  Max: 25.818 ms
  Avg: 22.904 ms

Avg Speedup: 49.63x

=====================================================
=== Attention Mechanism Benchmark ===
Configuration: B=8, nh=12, N=256, d=64
Total Q size: 6 MB

=== RESULTS ===
Naive Attention:
  Min: 695.662 ms
  Max: 697.210 ms
  Avg: 696.255 ms
Flash Attention:
  Min: 18.996 ms
  Max: 23.871 ms
  Avg: 20.493 ms

Avg Speedup: 33.98x

=====================================================
=== Attention Mechanism Benchmark ===
Configuration: B=8, nh=16, N=256, d=64
Total Q size: 8 MB

=== RESULTS ===
Naive Attention:
  Min: 1137.516 ms
  Max: 1138.145 ms
  Avg: 1137.854 ms
Flash Attention:
  Min: 21.469 ms
  Max: 25.810 ms
  Avg: 23.049 ms

Avg Speedup: 49.37x

Configuration: B=8, nh=8, N=64, d=64
Configuration: B=16, nh=8, N=64, d=64
Configuration: B=32, nh=8, N=64, d=64

Configuration: B=8, nh=12, N=64, d=64
Configuration: B=16, nh=12, N=64, d=64
Configuration: B=32, nh=12, N=64, d=64

Configuration: B=8, nh=16, N=64, d=64
Configuration: B=16, nh=16, N=64, d=64
Configuration: B=32, nh=16, N=64, d=64

Configuration: B=8, nh=12, N=128, d=64
Configuration: B=16, nh=12, N=128, d=64

Configuration: B=8, nh=8, N=256, d=64
Configuration: B=16, nh=8, N=256, d=64
Configuration: B=8, nh=12, N=256, d=64
Configuration: B=8, nh=16, N=256, d=64

./benchmark_attn 8 8 64 64